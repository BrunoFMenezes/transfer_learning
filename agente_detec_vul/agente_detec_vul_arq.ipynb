{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Agente para Detecção de Vulnerabilidades em Arquiteturas"
      ],
      "metadata": {
        "id": "rrEJdhbJqE1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo está um plano técnico conciso + exemplo mínimo de implementação (FastAPI) que satisfaz o desafio: receber imagem de diagrama arquitetural, extrair informação visual/textual, aplicar prompt engineering e gerar análise STRIDE estruturada via Azure OpenAI. Incluo dependências, arquitetura e um prompt template prático. Mantive o código reduzido — pronto para expandir conforme seu pipeline."
      ],
      "metadata": {
        "id": "oQN95jupqAKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 — Arquitetura proposta (resumo)\n",
        "\n",
        "FastAPI — endpoint POST /analyze recebe multipart/form-data com imagem.\n",
        "\n",
        "Azure AI Vision (Computer Vision) — (a) OCR / Read API para extrair texto do diagrama; (b) Image Analysis (caption, tags, objects) para obter descrição e entidades visuais.\n",
        "\n",
        "Preprocessing / Prompt engineering — combinar: (i) legenda/caption, (ii) texto OCR, (iii) lista de objetos/tags detectados, (iv) heurísticas (trust boundaries, componentes identificados) em um prompt template estruturado.\n",
        "\n",
        "Azure OpenAI — enviar prompt (Responses / Chat completions) pedindo análise STRIDE por componente e saída em JSON estruturado.\n",
        "\n",
        "Resposta — FastAPI retorna JSON com: metadados da imagem, itens OCR, itens visuais, prompt usado e análise STRIDE (por componente + recomendações de mitigação)."
      ],
      "metadata": {
        "id": "kFcnPm0fqRPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 — Dependências (mínimo)\n",
        "\n",
        "fastapi, uvicorn, python-multipart\n",
        "\n",
        "requests (ou azure-ai-vision e azure-ai-openai se preferir SDKs oficiais)\n",
        "\n",
        "Pillow (manipulação de imagem)\n",
        "(Recomendo usar SDKs oficiais Azure para produção.)"
      ],
      "metadata": {
        "id": "yPUob_w5qTWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 — Exemplo mínimo (FastAPI) — esqueleto funcional"
      ],
      "metadata": {
        "id": "7aMVGEyEqV01"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uw06ZSlAp_f4"
      },
      "outputs": [],
      "source": [
        "# requirements:\n",
        "# pip install fastapi uvicorn python-multipart requests Pillow\n",
        "\n",
        "import os, json, io\n",
        "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# ENV vars (configurar antes): AZ_VISION_ENDPOINT, AZ_VISION_KEY, AZ_OPENAI_ENDPOINT, AZ_OPENAI_KEY, AZ_OPENAI_DEPLOYMENT\n",
        "AZ_VISION_ENDPOINT = os.getenv(\"AZ_VISION_ENDPOINT\")\n",
        "AZ_VISION_KEY = os.getenv(\"AZ_VISION_KEY\")\n",
        "AZ_OPENAI_ENDPOINT = os.getenv(\"AZ_OPENAI_ENDPOINT\")  # ex: https://<resource>.openai.azure.com/\n",
        "AZ_OPENAI_KEY = os.getenv(\"AZ_OPENAI_KEY\")\n",
        "AZ_OPENAI_DEPLOYMENT = os.getenv(\"AZ_OPENAI_DEPLOYMENT\")  # deployment name\n",
        "\n",
        "if not all([AZ_VISION_ENDPOINT, AZ_VISION_KEY, AZ_OPENAI_ENDPOINT, AZ_OPENAI_KEY, AZ_OPENAI_DEPLOYMENT]):\n",
        "    raise RuntimeError(\"Configure AZ_VISION_*, AZ_OPENAI_* env vars\")\n",
        "\n",
        "# ---- Helpers: call Azure Vision Read (OCR) and Image Analyze (caption/tags) ----\n",
        "def vision_read_image_bytes(image_bytes: bytes):\n",
        "    # Read API (v3.2 or latest) - synchronous simple example\n",
        "    url = AZ_VISION_ENDPOINT.rstrip(\"/\") + \"/vision/v3.2/read/analyze\"\n",
        "    headers = {\"Ocp-Apim-Subscription-Key\": AZ_VISION_KEY, \"Content-Type\": \"application/octet-stream\"}\n",
        "    r = requests.post(url, headers=headers, data=image_bytes)\n",
        "    r.raise_for_status()\n",
        "    # Get operation-location to poll\n",
        "    op_url = r.headers.get(\"Operation-Location\")\n",
        "    # poll until done (simple loop, add timeout in prod)\n",
        "    resp = requests.get(op_url, headers={\"Ocp-Apim-Subscription-Key\": AZ_VISION_KEY})\n",
        "    while resp.json().get(\"status\") in (\"running\", \"notStarted\"):\n",
        "        import time; time.sleep(0.5)\n",
        "        resp = requests.get(op_url, headers={\"Ocp-Apim-Subscription-Key\": AZ_VISION_KEY})\n",
        "    # parse lines\n",
        "    lines = []\n",
        "    for readResult in resp.json().get(\"analyzeResult\", {}).get(\"readResults\", []):\n",
        "        for line in readResult.get(\"lines\", []):\n",
        "            lines.append(line.get(\"text\"))\n",
        "    return lines\n",
        "\n",
        "def vision_analyze_image_bytes(image_bytes: bytes):\n",
        "    # Image Analysis: get caption and tags (Image Analysis 4.0 example)\n",
        "    url = AZ_VISION_ENDPOINT.rstrip(\"/\") + \"/vision/v4.0/analyze?visualFeatures=Categories,Description,Tags,Objects\"\n",
        "    headers = {\"Ocp-Apim-Subscription-Key\": AZ_VISION_KEY, \"Content-Type\": \"application/octet-stream\"}\n",
        "    r = requests.post(url, headers=headers, data=image_bytes)\n",
        "    r.raise_for_status()\n",
        "    j = r.json()\n",
        "    caption = None\n",
        "    if \"description\" in j and j[\"description\"].get(\"captions\"):\n",
        "        caption = j[\"description\"][\"captions\"][0][\"text\"]\n",
        "    tags = [t[\"name\"] for t in j.get(\"tags\",[])]\n",
        "    objects = [o[\"object\"] for o in j.get(\"objects\",[])]\n",
        "    return {\"caption\": caption, \"tags\": tags, \"objects\": objects, \"raw\": j}\n",
        "\n",
        "# ---- Prompt engineering: template ----\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are a security analyst. Input below describes an application architecture diagram (extracted caption, OCR text, detected objects).\n",
        "Task: Produce a STRIDE threat analysis per identified component.\n",
        "Output: JSON with keys: 'components': [{ 'name':..., 'evidence':..., 'stride': { 'Spoofing': [...], 'Tampering': [...], 'Repudiation': [...], 'InfoDisclosure': [...], 'DoS': [...], 'Elevation': [...] }, 'recommendations': [...] }]\n",
        "\n",
        "Diagram metadata:\n",
        "CAPTION: {caption}\n",
        "OCR_TEXT: {ocr_text}\n",
        "DETECTED_OBJECTS: {objects}\n",
        "ADDITIONAL_NOTES: {notes}\n",
        "\n",
        "Rules:\n",
        "- Identify components (e.g., \"Load Balancer\", \"Auth Service\", \"Database\", \"API Gateway\", \"Mobile Client\") using the evidence fields.\n",
        "- For each component, produce at least one threat item per applicable STRIDE category (omit category only if clearly N/A).\n",
        "- Provide concise mitigation recommendations (3-6 words each).\n",
        "- Output strictly valid JSON only.\n",
        "\"\"\"\n",
        "\n",
        "# ---- Azure OpenAI call (ChatCompletion style using openai lib or REST) ----\n",
        "def call_azure_openai(prompt: str):\n",
        "    # simple REST call using OpenAI-compatible endpoint (adjust api-version if needed)\n",
        "    url = AZ_OPENAI_ENDPOINT.rstrip(\"/\") + f\"/openai/deployments/{AZ_OPENAI_DEPLOYMENT}/chat/completions?api-version=2023-03-15-preview\"\n",
        "    headers = {\"api-key\": AZ_OPENAI_KEY, \"Content-Type\": \"application/json\"}\n",
        "    body = {\n",
        "      \"messages\":[{\"role\":\"system\",\"content\":\"You are a security analyst.\"},\n",
        "                  {\"role\":\"user\",\"content\":prompt}],\n",
        "      \"max_tokens\":800,\n",
        "      \"temperature\":0.0\n",
        "    }\n",
        "    r = requests.post(url, headers=headers, json=body)\n",
        "    r.raise_for_status()\n",
        "    resp = r.json()\n",
        "    # extract assistant content (depends on model response shape)\n",
        "    return resp[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "# ---- FastAPI endpoint ----\n",
        "class AnalyzeResponse(BaseModel):\n",
        "    caption: str | None\n",
        "    ocr_text: list\n",
        "    detected_objects: list\n",
        "    stride_analysis: dict\n",
        "\n",
        "@app.post(\"/analyze\", response_model=AnalyzeResponse)\n",
        "async def analyze_arch_image(file: UploadFile = File(...)):\n",
        "    data = await file.read()\n",
        "    try:\n",
        "        ocr_lines = vision_read_image_bytes(data)\n",
        "        analysis = vision_analyze_image_bytes(data)\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "    caption = analysis.get(\"caption\")\n",
        "    objects = analysis.get(\"objects\",[])\n",
        "    prompt = PROMPT_TEMPLATE.format(caption=caption or \"\", ocr_text=\"\\\\n\".join(ocr_lines), objects=\", \".join(objects), notes=\"Use STRIDE\")\n",
        "    try:\n",
        "        ai_out = call_azure_openai(prompt)\n",
        "        # expect JSON string; try parse\n",
        "        stride_json = json.loads(ai_out)\n",
        "    except Exception as e:\n",
        "        # return raw AI output if JSON parsing fails\n",
        "        stride_json = {\"raw_ai_output\": ai_out, \"parse_error\": str(e)}\n",
        "    return {\"caption\": caption, \"ocr_text\": ocr_lines, \"detected_objects\": objects, \"stride_analysis\": stride_json}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notas sobre o exemplo\n",
        "\n",
        "O código usa Azure Vision Read e Image Analyze via REST; em produção prefira SDKs oficiais (azure-ai-vision, azure-ai-openai) com autenticação robusta.\n",
        "\n",
        "Ajuste api-version e deployment conforme sua subscrição. A URL/versão de API pode mudar: ver docs oficiais.\n",
        "\n",
        "O prompt template é a peça central: inclua evidências (nomes de componentes extraídos por OCR ou objetos detectados) para reduzir alucinações."
      ],
      "metadata": {
        "id": "5dM-Oq-UqbC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 — Dicas de Prompt Engineering (práticas)\n",
        "\n",
        "Forneça evidência estruturada: caption + OCR + tags em campos separados.\n",
        "\n",
        "Peça saída estruturada (JSON) e valide o JSON; repetir a instrução “Output strictly valid JSON only.” reduz prob. de formato livre.\n",
        "\n",
        "Temperatura baixa (0.0–0.2) para análises factuais e consistentes.\n",
        "\n",
        "Adicionar exemplos (few-shot): inclua 1 exemplo de entrada→JSON STRIDE para ensinar o formato.\n",
        "\n",
        "Verificação pós-processamento: validar se todas categorias STRIDE existem e adicionar placeholders se ausentes."
      ],
      "metadata": {
        "id": "CS5O8QjAqc8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 — Observações, riscos e recomendações técnicas\n",
        "\n",
        "Risco de hallucination do LLM ao identificar componentes não explicitamente presentes — mitigar: augment prompt with concrete OCR/object evidence e exigir evidence em cada componente.\n",
        "\n",
        "Privacidade: imagens arquiteturais podem conter segredos; garanta políticas de retenção/encryption e uso de redes privadas (VNet).\n",
        "\n",
        "Limitações: visão automática pode não reconhecer simbologia customizada; indique ao usuário requisito de mapear símbolos padrão (legenda) ou permita upload de metadados (JSON) junto com imagem.\n",
        "\n",
        "Validação humana: adicione etapa de revisão manual antes de aplicar mitigação em produção."
      ],
      "metadata": {
        "id": "vPwabQUUqfJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6 — Referências (obrigatórias)\n",
        "\n",
        "Azure OpenAI / Responses & Chat Quickstarts — Microsoft Learn.\n",
        "Microsoft Learn\n",
        "\n",
        "Azure Computer Vision — Read (OCR) & Image Analysis quickstarts.\n",
        "Microsoft Learn\n",
        "\n",
        "STRIDE threat model — Microsoft / referência STRIDE (definição e categorias).\n",
        "Microsoft Learn"
      ],
      "metadata": {
        "id": "J1-xc6YrqhWN"
      }
    }
  ]
}